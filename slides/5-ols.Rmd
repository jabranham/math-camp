---
title: "Ordinary Least Squares"
author: "J. Alexander Branham"
date: "Fall 2016"
header-includes: 
  - \usetheme[titleformat=smallcaps, progressbar=frametitle]{metropolis}
output: 
  beamer_presentation:
    fig_caption: yes
    incremental: yes
    slide_level: 3
    latex_engine: xelatex
classoption: aspectratio=169
---

```{r, echo=FALSE}
library(ggplot2)
```

# OLS in Matrix Form
## Notation
### Notation
* Let's pretend that we know the **true** model
    + $Y$ is $nx1$ column vector
    + $X$ is $nxk(+1)$ matrix 
    + $\beta$ is $kx1$ column vector
    + $E$ is $nx1$ column vector
* Therefore, we have:
\pause

$$
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}
= 
\begin{bmatrix}
1 & x_{11} & x_{12} & \ldots & x_{1k} \\
1 & x_{21} & x_{22} & \ldots & x_{2k} \\
1 & \vdots & \vdots & \ldots & \vdots \\
1 & x_{n1} & x_{n2} & \ldots & x_{nk}
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_k
\end{bmatrix}
+ 
\begin{bmatrix}
e_1 \\
e_2 \\
\vdots \\
e_n
\end{bmatrix}
$$

### Matrix form
$$
Y = X\beta + E
$$

## OLS Estimates
### OLS **minimizes the sum of squared residuals**
* How to do that in matrix form? 
* First, what is sum of squared residuals?
* The residuals:
$$E = Y - X \hat{\beta}$$
* Sum of squared residuals:
$$E'E$$
* (show why on board)
* Alternatively, 
$$
\begin{split}
E'E & = (Y - X \hat{\beta})'(Y - X \hat{\beta}) \\
 & = Y'Y - \hat{\beta} X' Y - Y' X \hat{\beta} + \hat{\beta}' X'X \hat{\beta} \\
 & = Y'Y - 2 \hat{\beta}' X' Y + \hat{\beta} X'X \hat{\beta}
\end{split}
$$

### To **minimize** the sum of squares, we take the derivative
* Remember: 
$$E'E= Y'Y - 2 \hat{\beta}' X' Y + \hat{\beta} X'X \hat{\beta}$$
* The first derivative with respect to $\hat{\beta}$
$$\dfrac{\partial E'E}{\partial \hat{\beta}} = -2 X'Y + 2X'X\hat{\beta} = 0 $$
* To check that this is a minimum, we check to make sure that the second derivative is positive
* The second derivative is $2X'X$, which is positive definite so long as $X$ is full rank

### Solve for the estimator
* Here ya go:
$$-2 X'Y + 2X'X\hat{\beta} = 0 $$
* Move things around and divide by two:
$$X'Y =  X'X \hat{\beta}$$
* Premultiply each side by $(X'X)^{-1}$
$$(X'X)^{-1}X'Y = (X'X)^{-1}(X'X)\hat{\beta}$$
* We know that $(X'X)^{-1}(X'X)=I$
$$(X'X)^{-1}X'Y = I\hat{\beta}$$
* And $I$ is (kinda) like multiplying by 1 so :
$$(X'X)^{-1}X'Y = \hat{\beta}$$

